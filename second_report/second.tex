\documentclass[12pt]{article}


\usepackage[compact]{titlesec} 
\usepackage{titling}
\usepackage[parfill]{parskip}


\posttitle{\par\end{center}\vskip 0.5em}
\posttitle{\par\end{center}}
\setlength{\droptitle}{-10pt}

\oddsidemargin0cm
\topmargin-2cm     
\textwidth16.5cm   
\textheight23.5cm  


\title{NLP First Project Report}
\date{\today}
\author{Aditya Gabbita, Evan Palmer, Kavya Srinet}
\begin{document}

\maketitle

\section{Toolkits used and tested}

We started by using the BeautifulSoup 4.2 module for scraping off the mark-up from Wikipedia articles, and extracted text between the <p> tags for obtaining the content of the article. But for the current development phase, we switched to the Wikipedia module for Python 2.7, which provides the basic functionality to obtain the content and relevant sections of Wikipedia articles remotely. After extracting the sections and content from the articles, we used the NLTK 3.0 module to parse the text into sentences for further processing.

The NLTK module was extended by downloading the Penn Tree Bank corpus, and the Stanford Parser jar files. We then used the nltk corpus to remove the stop words from the text, as a pre-processing step before analyzing the text for Question Answering. The questions for Wikipedia articles from Assignment 1 of the team were used as the baseline tests in the development of the first version of the Answering module. We imported the scikit module for Python to derive the tf-idf statistics for each question (making an assumption that each sentence in the article is an independent document).

For the Question Generation phase, we imported the Stanford Parser jar files into our module and analyzed the trees generated by the parser for sentences in the article. The Stanford Parser’s output can be read as a list into the Python module, and we can then convert it into a hierarchical tree which will aid us in generating questions by replacing the central subject and rearranging the Parts of Speech to form a fluent question.

We have set up a GITHUB repository (private), and have committed the first version of the answering module to the repository. The dependencies (external modules) have been added to the repository, after we decided a uniform convention for paths and the environments.

\section{Answering Module}

The following approach is used to in the current version of the answering module:
\begin{enumerate}
\item Preprocess the question and remove stopwords.
\item Get all the sentences from the document, and preprocess them as well.
\item Now we use the TfidfTransformer from sklearn to compute the tf-idf score of each word in the question in each sentence of the document.
\item We then pick all the sentences with the highest score (if there are more than one sentence for the maximum score).
\end{enumerate}

We used the ‘stopwords’ from nltk.corpus to obtain the list of all stop words in English. This approach enables us to retrieve the one (or more) sentence that contains the answer to our question.

As an add-on we tried to use nltk to do Named Entity Resolution and output the Proper noun based on the question, but it fetches us all Proper nouns in the relevant sentence. We need to work on how to find the important answer bearing entities and distinguish them from others.

As of now , our system works very well for easy, medium, hard and some very hard questions, and gives an accuracy of about 93% on these, where accuracy is computed based on the relevant answer bearing sentence retrieved.

We have attached the output of our program at the end of the report.

\textbf{Next Steps:}

\begin{itemize}
\item In the current version, our module cannot handle pronoun resolution, and cannot figure out the entity that the pronouns represent. We aim to incorporate this by the next progress review for the project.
\item Our implementation only retrieves the exact answer bearing sentence. We aim to retrieve the exact set of words that answer the question succinctly.
\item Handling True/False questions is another task that we need to work on. We plan to analyze the count of negative indicators in the relevant sentence, for e.g. if the count is odd, we flip the positive meaning of the sentence, and if it is even , we retain the positive meaning. We hope that this approach could help us answer true/false questions.
\item Another important aspect we we aim to address is to try and improve our answering module using question analysis. We can process the question to figure out whether it is a ‘What’, ’When’, ‘Who’ or ‘True/False’ type of question. Now based on the type of answer the question expects, we can use WordNet to validate whether or not our answer is correct before we output it with high confidence. For example, a “Who?” question should be answered with a Person’s name, a “When?” question should be answered with a Date and Time.
\item We also plan to use a stemmer to stem different morphologies of the same word, that should improve our system significantly and prevent us from dealing with rare morphological transformations of words with the same stem.
\item We would convert the text to a uniform case, to increase the efficiency of our TfIdfTransform and append a Named Entity Recognizer, to search for the important entities in the text.
\item We aim to standardize formats of representation of entities like Date and Time, as the format in the question may not align with that in the document. And this would resolve the difficulties arising from direct string matching in tf-idf.
\item We finally would leverage features of Wordnet, to get antonyms and synonyms of words, to be able to answer the questions better. We can also leverage the class of cities/places etc. to confirm whether our answer is correct or not, after performing question analysis.
\end{itemize}

\section{Questioning Module} % (Evan)

Most of the work we have done so far has been on Question Answering, but we will include some discussion of our plan for Question Generation.

As opposed to question answering, question generation does not appear to have a simple to implement baseline solution. 

\textbf{Planned Steps:}

\begin{enumerate}
\item First we will need to simple factual statements from the text so that we can create questions base on them. This will likely entail looking through the parse tree generated by the stanford parser for certain constructs. We will need to determine the type of constructs which are likely to contain factual statements.
\item After broken the text up into small factual statements, we will need to resolve pronouns in all of the factual statements we plan to use. In this process we can discard some types of statements if pronoun resolution performs poorly, or has a low confidence level.
\item After this stage we will have small factual statements, each where at least the first occurrence of every entity in the question is not a pronoun. From here, we will select an answer word or phrase, and identify what type of entity it refers to (person, place, time, count, etc.). The identification step could be accomplished with NER, or something like the SuperSense Tagger (which is based on WordNet) which will identify regular nouns and verbs in addition to named entities. 
\item Once we have identified an answer phrase and phrase type for a fact, we can replace the answer with its related question word (who, what, how many, etc.) and attempt to rearrange the sentence appropriately.
\end{enumerate}

\section{Questions and output so far}

\subsection{Topic: Esperanto}

Q1: Has any country adopted Esperanto officially? \\
Although no country has adopted Esperanto officially, Esperanto was recommended by the French Academy of Sciences in 1921 and recognized by UNESCO in 1954, which recommended to international non-government organizations to use Esperanto in 1985.

Q2 : Where was the first book of Esperanto grammar published? \\
After some ten years of development, which Zamenhof spent translating literature into Esperanto as well as writing original prose and verse, the first book of Esperanto grammar was published in Warsaw on the 26th of July 1887.

Q3: Was Esperanto widely used by radio broadcasters? \\
In the summer of 1924, the American Radio Relay League adopted Esperanto as its official international auxiliary language, and hoped that the language would be used by radio amateurs in international communications, but its actual use for radio communications was negligible.

Q4: What was the language of the 1965 B-movie horror file Incubus? \\
Two full-length feature films have been produced with dialogue entirely in Esperanto: Angoroj, in 1964, and Incubus, a 1965 B-movie horror film.


\subsection{Topic: Kaval} 

Q1: Where is Kaval used? \\
The kaval is primarily associated with mountain shepherds throughout the Balkans and Anatolia and in the book Kaval: Traditional Folk Melodies for Balkan \& Anatolian Folk Flute, musician Pat MacSwyney suggest that the kaval spread with the Yoruks from the Taurus mountains of southern Anatolia into the southern Balkans of southeast Europe.
Dilli Kaval


% \subsection{Topic: Budapest}

% Q1: Where was Budapest ranked on Mastercard's Emerging Markets index? \\
% Considered a financial hub in Central Europe, the city ranked 3rd on Mastercard's Emerging Markets Index, and ranked as the most liveable Central or Eastern European city on EIU's quality of life index.

\subsection{Topic: Santiago}

Q1: What is the name of Santiago's public bus transport system? \\
Transantiago is the name for the city's public transport system.

Q2: Are the eastern communes in Santiago richer than the western communes? \\
The western half (zona poniente) of the city is, on average, much poorer than the eastern communes, where the high-standard public and private facilities are concentrated.

Q3: Has the air pollution in Santiago reduced in 2010? \\
A study by a Chilean university found in 2010 that Santiago pollution had doubled.

\subsection{Topic: Crux}

Q1: Is the Crux easily visible from the Northern Hemisphere throughout the year? \\
Crux is easily visible from the southern hemisphere at practically any time of year.

Q2: Which constellation borders the Crux on east, north and west? \\
Crux is bordered by the constellations Centaurus (which surrounds it on three sides) on the east, north and west, and Musca to the south.

Q3: what is the most prominent dark nebula in the skies? \\
The Coalsack Nebula is the most prominent dark nebula in the skies, easily visible to the naked eye as a prominent dark patch in the southern Milky Way.

Q4: Is it commonly known as the Southern Cross? \\
Its name is Latin for cross, and it is dominated by a cross-shaped asterism that is commonly known as the Southern Cross.


\end{document}